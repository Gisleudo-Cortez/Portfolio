{"cells":[{"source":"![servicedesk](servicedesk.png)\n\nCleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can automatically categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as mortgage, credit card, money transfers, debt collection, etc.","metadata":{"executionCancelledAt":null,"executionTime":165,"lastExecutedAt":1707667023665,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"CleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can autonomously categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as technical issues, billing inquiries, cancellation requests, refunds, and product information requests."},"id":"e5870ae0-6165-459e-9c40-0f282883be7b","cell_type":"markdown"},{"source":"!pip install torchmetrics","metadata":{"executionCancelledAt":null,"executionTime":3261,"lastExecutedAt":1731496484441,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install torchmetrics","outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"10b36e79-f9d2-48b8-91d7-09f443f672f9"},"id":"0dd4beb4-2329-4b0d-8a34-2354ee9c7fb4","cell_type":"code","execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torchmetrics in /home/repl/.local/lib/python3.8/site-packages (1.5.2)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.23.2)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (23.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /home/repl/.local/lib/python3.8/site-packages (from torchmetrics) (0.11.8)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.9.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.6.3)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->torchmetrics) (0.38.4)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"}]},{"source":"from collections import Counter\nimport nltk, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1731496484494,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from collections import Counter\nimport nltk, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall","lastExecutedByKernel":"10b36e79-f9d2-48b8-91d7-09f443f672f9"},"id":"2fa90b61-0244-4236-aa93-e33a7a088eec","cell_type":"code","execution_count":92,"outputs":[]},{"source":"nltk.download('punkt')","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1731496484545,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"nltk.download('punkt')","outputsMetadata":{"0":{"height":80,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"10b36e79-f9d2-48b8-91d7-09f443f672f9"},"id":"37a51a81-1301-4a80-b8c6-716faaff4c5c","cell_type":"code","execution_count":93,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /home/repl/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":93}]},{"source":"# Import data and labels\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)\nlabels = np.load('labels.npy')","metadata":{"executionCancelledAt":null,"executionTime":148,"lastExecutedAt":1731496484693,"lastExecutedByKernel":"10b36e79-f9d2-48b8-91d7-09f443f672f9","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import data and labels\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)\nlabels = np.load('labels.npy')"},"id":"e1b12eaf-e55c-422c-94a2-b0197c465a1b","cell_type":"code","execution_count":94,"outputs":[]},{"source":"# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}\n\n# Looking up the mapping dictionary and assigning the index to the respective words\nfor i, sentence in enumerate(text):\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n    \n# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ntext = pad_input(text, 50)","metadata":{"executionCancelledAt":null,"executionTime":290,"lastExecutedAt":1731496484984,"lastExecutedByKernel":"10b36e79-f9d2-48b8-91d7-09f443f672f9","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}\n\n# Looking up the mapping dictionary and assigning the index to the respective words\nfor i, sentence in enumerate(text):\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n    \n# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ntext = pad_input(text, 50)"},"id":"d630badb-23dd-4368-9a96-e2b478ad5cff","cell_type":"code","execution_count":95,"outputs":[]},{"source":"# Splitting dataset\ntrain_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\n# Modified\n# Create DataLoaders\nbatch_size = 32  # Added batch size\ntrain_data = DataLoader(\n    TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long()),\n    batch_size=batch_size,\n    shuffle=True\n)\ntest_data = DataLoader(\n    TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long()),\n    batch_size=batch_size\n)","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1731496485035,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Splitting dataset\ntrain_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\n# Modified\n# Create DataLoaders\nbatch_size = 32  # Added batch size\ntrain_data = DataLoader(\n    TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long()),\n    batch_size=batch_size,\n    shuffle=True\n)\ntest_data = DataLoader(\n    TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long()),\n    batch_size=batch_size\n)","lastExecutedByKernel":"10b36e79-f9d2-48b8-91d7-09f443f672f9"},"id":"f2654836-631f-415e-9922-5ab3bafaaafa","cell_type":"code","execution_count":96,"outputs":[]},{"source":"## Define a CNN classifier","metadata":{},"cell_type":"markdown","id":"b7193bb0-4b84-4618-b2b0-b69e8d9b50cf"},{"source":"class CNNClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=128, num_classes=10):\n        super(CNNClassifier, self).__init__()\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        \n        # Convolutional layer\n        self.conv1d = nn.Conv1d(embed_dim, 256, kernel_size=3, padding=1)\n        \n        # Calculate the size of the flattened features\n        self.flatten_size = 256 * 25  # After max pooling with kernel_size=2\n        \n        # Linear layer\n        self.fc1 = nn.Linear(self.flatten_size, num_classes)\n    \n    def forward(self, x):\n        # Embedding layer: [batch_size, seq_len] -> [batch_size, seq_len, embed_dim]\n        embedded = self.embedding(x)\n        \n        # Transpose for conv1d: [batch_size, embed_dim, seq_len]\n        embedded = embedded.transpose(1, 2)\n        \n        # Convolutional layer\n        conv_out = self.conv1d(embedded)\n        conv_out = F.relu(conv_out)\n        \n        # Max pooling\n        pooled = F.max_pool1d(conv_out, kernel_size=2)\n        \n        # Flatten\n        flattened = pooled.view(-1, self.flatten_size)\n        \n        # Linear layer\n        output = self.fc1(flattened)\n        \n        return output","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1731496485090,"lastExecutedByKernel":"10b36e79-f9d2-48b8-91d7-09f443f672f9","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"class CNNClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim=128, num_classes=10):\n        super(CNNClassifier, self).__init__()\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        \n        # Convolutional layer\n        self.conv1d = nn.Conv1d(embed_dim, 256, kernel_size=3, padding=1)\n        \n        # Calculate the size of the flattened features\n        self.flatten_size = 256 * 25  # After max pooling with kernel_size=2\n        \n        # Linear layer\n        self.fc1 = nn.Linear(self.flatten_size, num_classes)\n    \n    def forward(self, x):\n        # Embedding layer: [batch_size, seq_len] -> [batch_size, seq_len, embed_dim]\n        embedded = self.embedding(x)\n        \n        # Transpose for conv1d: [batch_size, embed_dim, seq_len]\n        embedded = embedded.transpose(1, 2)\n        \n        # Convolutional layer\n        conv_out = self.conv1d(embedded)\n        conv_out = F.relu(conv_out)\n        \n        # Max pooling\n        pooled = F.max_pool1d(conv_out, kernel_size=2)\n        \n        # Flatten\n        flattened = pooled.view(-1, self.flatten_size)\n        \n        # Linear layer\n        output = self.fc1(flattened)\n        \n        return output"},"cell_type":"code","id":"ff8ed9dd-d7be-4a87-86b8-955418edf775","outputs":[],"execution_count":97},{"source":"## Train your classifier","metadata":{},"cell_type":"markdown","id":"681e3b53-7462-4f79-846f-3db30b2f0d7d"},{"source":"# Initialize the model\nmodel = CNNClassifier(vocab_size=len(word2idx))\n\n# Define the loss function and the optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 3\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    \n    for batch_x, batch_y in train_data:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        \n        # Zero out the gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(batch_x)\n        loss = criterion(output, batch_y)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(train_data)\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')","metadata":{"executionCancelledAt":null,"executionTime":18121,"lastExecutedAt":1731496503213,"lastExecutedByKernel":"10b36e79-f9d2-48b8-91d7-09f443f672f9","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Initialize the model\nmodel = CNNClassifier(vocab_size=len(word2idx))\n\n# Define the loss function and the optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 3\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    \n    for batch_x, batch_y in train_data:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        \n        # Zero out the gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(batch_x)\n        loss = criterion(output, batch_y)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(train_data)\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"b0bd7df8-53b3-48a2-a979-6486092174d0","outputs":[{"output_type":"stream","name":"stdout","text":"Epoch 1/3, Loss: 1.3741\nEpoch 2/3, Loss: 0.4679\nEpoch 3/3, Loss: 0.1640\n"}],"execution_count":98},{"source":"## Test your classifier","metadata":{},"cell_type":"markdown","id":"2abdb925-bbc0-4352-8bf6-92f42a05d422"},{"source":"# Evaluation\nmodel.eval()\naccuracy = Accuracy(task=\"multiclass\", num_classes=10).to(device)\nprecision = Precision(task=\"multiclass\", num_classes=10, average=None).to(device)\nrecall = Recall(task=\"multiclass\", num_classes=10, average=None).to(device)\n\n# Initialize empty lists for predictions and true labels\npredicted = []\ntrue_labels = []\n\n# Disable gradient computation for evaluation\nwith torch.no_grad():\n    for batch_x, batch_y in test_data:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        \n        # Get model predictions\n        outputs = model(batch_x)\n        _, p = torch.max(outputs, 1)\n        \n        # Store predictions and true labels\n        predicted.extend(p.cpu().numpy())\n        true_labels.extend(batch_y.cpu().numpy())","metadata":{"executionCancelledAt":null,"executionTime":217,"lastExecutedAt":1731496503430,"lastExecutedByKernel":"10b36e79-f9d2-48b8-91d7-09f443f672f9","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Evaluation\nmodel.eval()\naccuracy = Accuracy(task=\"multiclass\", num_classes=10).to(device)\nprecision = Precision(task=\"multiclass\", num_classes=10, average=None).to(device)\nrecall = Recall(task=\"multiclass\", num_classes=10, average=None).to(device)\n\n# Initialize empty lists for predictions and true labels\npredicted = []\ntrue_labels = []\n\n# Disable gradient computation for evaluation\nwith torch.no_grad():\n    for batch_x, batch_y in test_data:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        \n        # Get model predictions\n        outputs = model(batch_x)\n        _, p = torch.max(outputs, 1)\n        \n        # Store predictions and true labels\n        predicted.extend(p.cpu().numpy())\n        true_labels.extend(batch_y.cpu().numpy())"},"cell_type":"code","id":"6fa734e5-6ea9-43ba-bf8b-8278a32dae72","outputs":[],"execution_count":99},{"source":"## Calculate the accuracy, per-class precision, and recall","metadata":{},"cell_type":"markdown","id":"e9f5d3c8-a6ea-4d27-a09e-b16f2c082dea"},{"source":"# Convert to tensors for metric calculation\npredictions_tensor = torch.tensor(predictions).to(device)\nlabels_tensor = torch.tensor(true_labels).to(device)\n\n# Calculate accuracy\naccuracy = (predictions_tensor == labels_tensor).float().mean().item()\n\n# Initialize per-class metrics\nprecision_metric = Precision(task=\"multiclass\", num_classes=10, average=None).to(device)\nrecall_metric = Recall(task=\"multiclass\", num_classes=10, average=None).to(device)\n\n# Calculate precision and recall\nprecision = precision_metric(predictions_tensor, labels_tensor).cpu().tolist()\nrecall = recall_metric(predictions_tensor, labels_tensor).cpu().tolist()\n\n# Print results\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision per class: {precision}\")\nprint(f\"Recall per class: {recall}\")","metadata":{"executionCancelledAt":null,"executionTime":127,"lastExecutedAt":1731496503559,"lastExecutedByKernel":"10b36e79-f9d2-48b8-91d7-09f443f672f9","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Convert to tensors for metric calculation\npredictions_tensor = torch.tensor(predictions).to(device)\nlabels_tensor = torch.tensor(true_labels).to(device)\n\n# Calculate accuracy\naccuracy = (predictions_tensor == labels_tensor).float().mean().item()\n\n# Initialize per-class metrics\nprecision_metric = Precision(task=\"multiclass\", num_classes=10, average=None).to(device)\nrecall_metric = Recall(task=\"multiclass\", num_classes=10, average=None).to(device)\n\n# Calculate precision and recall\nprecision = precision_metric(predictions_tensor, labels_tensor).cpu().tolist()\nrecall = recall_metric(predictions_tensor, labels_tensor).cpu().tolist()\n\n# Print results\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision per class: {precision}\")\nprint(f\"Recall per class: {recall}\")","outputsMetadata":{"0":{"height":123,"type":"stream"}}},"cell_type":"code","id":"c688f0a4-63d4-4395-808a-8639271f83e5","outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy: 0.7010\nPrecision per class: [0.559183657169342, 0.7310344576835632, 0.7058823704719543, 0.7134831547737122, 0.8402062058448792, 0.0, 0.0, 0.0, 0.0, 0.0]\nRecall per class: [0.7135416865348816, 0.557894766330719, 0.7777777910232544, 0.6614583134651184, 0.776190459728241, 0.0, 0.0, 0.0, 0.0, 0.0]\n"}],"execution_count":100}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}