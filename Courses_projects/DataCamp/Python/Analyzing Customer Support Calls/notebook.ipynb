{"cells":[{"source":"<p align=\"center\" width=\"100%\">\n    <img width=\"40%\" src=\"customer_support_icon.JPG\"> \n</p>\n\nA retail company is on a transformative journey, aiming to elevate their customer services through cutting-edge advancements in Speech Recognition and Natural Language Processing (NLP). As the machine learning engineer for this initiative, you are tasked with developing functionalities that not only convert customer support audio calls into text but also explore methodologies to extract insights from transcribed texts.\n\nIn this dynamic project, we leverage the power of `SpeechRecognition`, `Pydub`, and `spaCy` – three open-source packages that form the backbone of your solution. Your objectives are:\n  - Transcribe a sample customer audio call, stored at `sample_customer_call.wav`, to showcase the power of open-source speech recognition technology.\n  - Analyze sentiment, identify common named entities, and enhance user experience by searching for the most similar customer calls based on a given query from a subset of their pre-transcribed call data, stored at `customer_call_transcriptions.csv`.\n\nThis project is an opportunity to unlock the potential of machine learning to revolutionize customer support. Let's delve into the interplay between technology and service excellence.","metadata":{},"id":"d5e81b43-ccfd-4fc6-902c-59cd49aa9913","cell_type":"markdown"},{"source":"!pip install SpeechRecognition\n!pip install pydub\n!pip install spacy\n!python3 -m spacy download en_core_web_sm","metadata":{"executionCancelledAt":null,"executionTime":18933,"lastExecutedAt":1731932100636,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install SpeechRecognition\n!pip install pydub\n!pip install spacy\n!python3 -m spacy download en_core_web_sm","outputsMetadata":{"0":{"height":613,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"cd2e20c4-bf35-4354-87ab-212d8128de9d"},"id":"d0f1598e-18a8-45d5-8387-bf2f5ce4ffd6","cell_type":"code","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: SpeechRecognition in /home/repl/.local/lib/python3.8/site-packages (3.10.4)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.8/dist-packages (from SpeechRecognition) (2.31.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from SpeechRecognition) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (2019.11.28)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pydub in /home/repl/.local/lib/python3.8/site-packages (0.25.1)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.6.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.0)\nRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.23.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.12)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.1.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (65.6.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.1.1)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nCollecting en-core-web-sm==3.6.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.64.0)\nRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (65.6.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2019.11.28)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n"}]},{"source":"# Import required libraries\nimport pandas as pd\n\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport speech_recognition as sr\nfrom pydub import AudioSegment\n\nimport spacy","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1731932100688,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nimport pandas as pd\n\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport speech_recognition as sr\nfrom pydub import AudioSegment\n\nimport spacy","outputsMetadata":{"0":{"height":77,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"cd2e20c4-bf35-4354-87ab-212d8128de9d"},"id":"d6f3dd61-8c75-48d4-b2a5-79cd0b444ddb","cell_type":"code","execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/repl/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"}]},{"source":"## Is the audio compatible for future speech recognition modeling?","metadata":{},"cell_type":"markdown","id":"12618254-3a02-4499-ad5a-20cd5f2ad5b4"},{"source":"# Load the speech recognition model and recognize the audio\nr = sr.Recognizer()\nwith sr.AudioFile('sample_customer_call.wav') as source:\n    audio_data = r.record(source)\n    transcribed_text = r.recognize_google(audio_data)\n\nprint(f\"Transcribed Text: {transcribed_text}\")","metadata":{"executionCancelledAt":null,"executionTime":1233,"lastExecutedAt":1731932101921,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load the speech recognition model and recognize the audio\nr = sr.Recognizer()\nwith sr.AudioFile('sample_customer_call.wav') as source:\n    audio_data = r.record(source)\n    transcribed_text = r.recognize_google(audio_data)\n\nprint(f\"Transcribed Text: {transcribed_text}\")","outputsMetadata":{"0":{"height":177,"type":"stream"}},"lastExecutedByKernel":"cd2e20c4-bf35-4354-87ab-212d8128de9d"},"id":"250524c2-1bd3-4ff8-a224-8fa007566c1b","cell_type":"code","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"Transcribed Text: hello I'm experiencing an issue with your product I'd like to speak to someone about a replacement\n"}]},{"source":"# Load audio file with PyDub to analyze frame rate and channels\naudio = AudioSegment.from_wav('sample_customer_call.wav')\n\n# Get the frame rate and number of channels\nframe_rate = audio.frame_rate\nnumber_channels = audio.channels\n\nprint(f\"Frame Rate: {frame_rate}\")\nprint(f\"Number of Channels: {number_channels}\")","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1731932101972,"lastExecutedByKernel":"cd2e20c4-bf35-4354-87ab-212d8128de9d","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load audio file with PyDub to analyze frame rate and channels\naudio = AudioSegment.from_wav('sample_customer_call.wav')\n\n# Get the frame rate and number of channels\nframe_rate = audio.frame_rate\nnumber_channels = audio.channels\n\nprint(f\"Frame Rate: {frame_rate}\")\nprint(f\"Number of Channels: {number_channels}\")"},"cell_type":"code","id":"2d4340f0-2da3-4c11-81f4-b4eddcab1990","outputs":[{"output_type":"stream","name":"stdout","text":"Frame Rate: 44100\nNumber of Channels: 1\n"}],"execution_count":14},{"source":"## How many calls have a true positive sentiment?","metadata":{},"cell_type":"markdown","id":"70499e86-d444-4fb3-9436-086d5b296b8b"},{"source":"# Load the dataset\ndata = pd.read_csv('customer_call_transcriptions.csv')\n\n# Initialize VADER sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\ndef analyze_sentiment(text):\n    score = sia.polarity_scores(text)['compound']\n    if score >= 0.05:\n        return 'positive'\n    elif score <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'\n\n# Apply sentiment analysis to the transcriptions\ndata['sentiment'] = data['text'].apply(analyze_sentiment)\n\n# Calculate true positives for positive sentiments\ntrue_positive = len(data[(data['sentiment'] == 'positive') & (data['sentiment_label'] == 'positive')])\n\nprint(f\"True Positive Predictions: {true_positive}\")","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1731932102024,"lastExecutedByKernel":"cd2e20c4-bf35-4354-87ab-212d8128de9d","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load the dataset\ndata = pd.read_csv('customer_call_transcriptions.csv')\n\n# Initialize VADER sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\ndef analyze_sentiment(text):\n    score = sia.polarity_scores(text)['compound']\n    if score >= 0.05:\n        return 'positive'\n    elif score <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'\n\n# Apply sentiment analysis to the transcriptions\ndata['sentiment'] = data['text'].apply(analyze_sentiment)\n\n# Calculate true positives for positive sentiments\ntrue_positive = len(data[(data['sentiment'] == 'positive') & (data['sentiment_label'] == 'positive')])\n\nprint(f\"True Positive Predictions: {true_positive}\")","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"967a24f2-4bca-4069-b9f6-b26f992d4549","outputs":[{"output_type":"stream","name":"stdout","text":"True Positive Predictions: 2\n"}],"execution_count":15},{"source":"## What is the most frequently named entity across all of the transcriptions?","metadata":{},"cell_type":"markdown","id":"7368c3c0-1c8f-4ed3-875a-c7f424bd0d04"},{"source":"nlp = spacy.load(\"en_core_web_sm\")\n\ndef find_most_freq_ent(transcriptions):\n    # Process all transcriptions with spaCy's NER\n    doc = nlp.pipe(transcriptions)\n\n    ent_counter = {}\n    for d in doc:\n        for ent in d.ents:\n            if ent.text in ent_counter:\n                ent_counter[ent.text] += 1\n            else:\n                ent_counter[ent.text] = 1\n\n    # Find the most frequent entity\n    most_freq_ent, freq = max(ent_counter.items(), key=lambda x: x[1])\n\n    return most_freq_ent, freq\n\n# Apply function to transcriptions and store result in variable\ntranscriptions = data['text'].tolist()\nmost_freq_ent, freq = find_most_freq_ent(transcriptions)\n\nprint(f\"Most frequently named entity: {most_freq_ent} (Frequency: {freq})\")","metadata":{"executionCancelledAt":null,"executionTime":633,"lastExecutedAt":1731932102657,"lastExecutedByKernel":"cd2e20c4-bf35-4354-87ab-212d8128de9d","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"nlp = spacy.load(\"en_core_web_sm\")\n\ndef find_most_freq_ent(transcriptions):\n    # Process all transcriptions with spaCy's NER\n    doc = nlp.pipe(transcriptions)\n\n    ent_counter = {}\n    for d in doc:\n        for ent in d.ents:\n            if ent.text in ent_counter:\n                ent_counter[ent.text] += 1\n            else:\n                ent_counter[ent.text] = 1\n\n    # Find the most frequent entity\n    most_freq_ent, freq = max(ent_counter.items(), key=lambda x: x[1])\n\n    return most_freq_ent, freq\n\n# Apply function to transcriptions and store result in variable\ntranscriptions = data['text'].tolist()\nmost_freq_ent, freq = find_most_freq_ent(transcriptions)\n\nprint(f\"Most frequently named entity: {most_freq_ent} (Frequency: {freq})\")"},"cell_type":"code","id":"66a236f3-59f0-43d1-8a4b-9be84ab49b03","outputs":[{"output_type":"stream","name":"stdout","text":"Most frequently named entity: yesterday (Frequency: 15)\n"}],"execution_count":16},{"source":"## Which call is the most similar to \"wrong package delivery\"?","metadata":{},"cell_type":"markdown","id":"fe373680-ec0a-4a83-b33d-5a4aed847b29"},{"source":"# Import required library for similarity calculation\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Initialize TfidfVectorizer and encode transcriptions and query\ntranscriptions = data['text'].tolist()\nvectorizer = TfidfVectorizer().fit_transform(transcriptions + [\"wrong package delivery\"])\nquery_vec = vectorizer[-1]\n\n# Calculate cosine similarity between query and all transcriptions\nsimilarity_scores = list(enumerate(cosine_similarity(query_vec, vectorizer[:len(transcriptions)]).flatten()))\n\n# Get the index of the most similar transcription\nmost_similar_idx, _ = max(similarity_scores, key=lambda x: x[1])\n\n# Save the most similar text as string variable\nmost_similar_text = data.iloc[most_similar_idx]['text']\n\nprint(f\"Most similar call to 'wrong package delivery':\\n{most_similar_text}\")","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1731932102708,"lastExecutedByKernel":"cd2e20c4-bf35-4354-87ab-212d8128de9d","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required library for similarity calculation\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Initialize TfidfVectorizer and encode transcriptions and query\ntranscriptions = data['text'].tolist()\nvectorizer = TfidfVectorizer().fit_transform(transcriptions + [\"wrong package delivery\"])\nquery_vec = vectorizer[-1]\n\n# Calculate cosine similarity between query and all transcriptions\nsimilarity_scores = list(enumerate(cosine_similarity(query_vec, vectorizer[:len(transcriptions)]).flatten()))\n\n# Get the index of the most similar transcription\nmost_similar_idx, _ = max(similarity_scores, key=lambda x: x[1])\n\n# Save the most similar text as string variable\nmost_similar_text = data.iloc[most_similar_idx]['text']\n\nprint(f\"Most similar call to 'wrong package delivery':\\n{most_similar_text}\")","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"97b564fb-ca72-46d4-98f5-0d1b95488cb6","outputs":[{"output_type":"stream","name":"stdout","text":"Most similar call to 'wrong package delivery':\nwrong package delivered\n"}],"execution_count":17}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}