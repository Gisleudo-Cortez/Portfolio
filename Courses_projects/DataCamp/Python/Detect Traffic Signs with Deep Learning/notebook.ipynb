{"cells":[{"source":"![Traffic lights over urban intersection.](traffic.jpg)","metadata":{},"id":"9bef1e92-37c0-4f79-a27a-74f017644cdf","cell_type":"markdown"},{"source":"**Challenges in Traffic Sign Management**\n\nTraffic signs convey vital information to drivers. However, their effectiveness can be hindered by various factors, such as their visibility under different lighting conditions or the presence of obstructions.\n\n\n**Training Traffic Sign Detection Models**\n\nTo address these challenges and enhance stop sign and traffic light detection capabilities, advanced technologies such as deep learning and computer vision have gained significant attention. In this project, you'll train an object detection model on 6 images of stop signs and 6 images of traffic lights, taken from various angles and lighting conditions. This training teaches the model to classify and locate these signs in images, improving its robustness and reliability for real-world applications.","metadata":{},"id":"1c2bc1d6-3780-4d6f-8016-c90898817887","cell_type":"markdown"},{"source":"# Import required libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\n\n# Load preprocessed images and the corresponding labels\nimage, labels = np.load('batch.npy',allow_pickle=True).tolist()\n\n# hyperparameters\ninput_size = image.shape[1] # dimension of input image\nnum_classes = labels['classifier_head'].shape[1] # number of classes\nDROPOUT_FACTOR = 0.2 # dropout probability\n\n# visualize one example preprocessed image\nplt.imshow(image[2])\nplt.axis(\"off\")","metadata":{"executionCancelledAt":null,"executionTime":5051,"lastExecutedAt":1731584880145,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\n\n# Load preprocessed images and the corresponding labels\nimage, labels = np.load('batch.npy',allow_pickle=True).tolist()\n\n# hyperparameters\ninput_size = image.shape[1] # dimension of input image\nnum_classes = labels['classifier_head'].shape[1] # number of classes\nDROPOUT_FACTOR = 0.2 # dropout probability\n\n# visualize one example preprocessed image\nplt.imshow(image[2])\nplt.axis(\"off\")","outputsMetadata":{"0":{"height":616,"type":"stream"},"1":{"height":185,"type":"stream"}},"lastExecutedByKernel":"041e41a6-a0a1-4d6b-89ae-5926b6ecaa91"},"id":"2f26fc58-2bcd-45a6-841f-cbcdd391e2f2","cell_type":"code","execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":"(-0.5, 223.5, 223.5, -0.5)"},"metadata":{},"execution_count":1},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{"image/png":{"width":389,"height":389}}}]},{"source":"## Define and train an object detection model to identify traffic signs and lights","metadata":{},"cell_type":"markdown","id":"6655a192-6403-4696-8d08-53c25d145eda"},{"source":"# Define the object detection model\nmodel = keras.Sequential([\n    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(input_size, input_size, 3)),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    keras.layers.Dropout(DROPOUT_FACTOR),\n    keras.layers.Flatten(),\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.Dropout(DROPOUT_FACTOR),\n    keras.layers.Dense(num_classes, activation='softmax')\n])","metadata":{"executionCancelledAt":null,"executionTime":936,"lastExecutedAt":1731584881081,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define the object detection model\nmodel = keras.Sequential([\n    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(input_size, input_size, 3)),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    keras.layers.Dropout(DROPOUT_FACTOR),\n    keras.layers.Flatten(),\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.Dropout(DROPOUT_FACTOR),\n    keras.layers.Dense(num_classes, activation='softmax')\n])","lastExecutedByKernel":"041e41a6-a0a1-4d6b-89ae-5926b6ecaa91","outputsMetadata":{"0":{"height":102,"type":"stream"},"1":{"height":616,"type":"stream"}}},"id":"84cdd85c-afb2-4f4f-b66c-8b158eeac8d2","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":"2024-11-14 11:48:00.162387: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"}]},{"source":"# Compile the model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(lr=0.001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1731584881134,"lastExecutedByKernel":"041e41a6-a0a1-4d6b-89ae-5926b6ecaa91","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Compile the model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(lr=0.001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)"},"cell_type":"code","id":"e67983d4-ba0e-4059-9afb-5fdcf8206f35","outputs":[],"execution_count":3},{"source":"# Train the model\nhistory = model.fit(\n    image,\n    labels['classifier_head'],\n    epochs=20,\n    validation_split=0.2,\n    verbose=1\n)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"26ffdd45-b7df-4f9e-9632-bec5b55aa818","outputs":[{"output_type":"stream","name":"stdout","text":"Epoch 1/20\n1/1 [==============================] - 3s 3s/step - loss: 7.2197 - accuracy: 0.5556 - val_loss: 4091.2031 - val_accuracy: 0.6667\nEpoch 2/20\n1/1 [==============================] - 2s 2s/step - loss: 3219.8867 - accuracy: 0.4444 - val_loss: 859.0146 - val_accuracy: 0.3333\nEpoch 3/20\n1/1 [==============================] - 2s 2s/step - loss: 575.1327 - accuracy: 0.6667 - val_loss: 454.5114 - val_accuracy: 0.3333\nEpoch 4/20\n1/1 [==============================] - 2s 2s/step - loss: 286.8004 - accuracy: 0.6667 - val_loss: 177.8972 - val_accuracy: 0.6667\nEpoch 5/20\n1/1 [==============================] - 2s 2s/step - loss: 33.4902 - accuracy: 0.8889 - val_loss: 428.3440 - val_accuracy: 0.6667\nEpoch 6/20\n1/1 [==============================] - 2s 2s/step - loss: 141.9735 - accuracy: 0.5556 - val_loss: 120.9516 - val_accuracy: 0.6667\nEpoch 7/20\n1/1 [==============================] - 2s 2s/step - loss: 21.9736 - accuracy: 0.8889 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\nEpoch 8/20\n1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 82.0954 - val_accuracy: 0.3333\nEpoch 9/20\n1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 226.9118 - val_accuracy: 0.3333\nEpoch 10/20\n1/1 [==============================] - 2s 2s/step - loss: 34.5459 - accuracy: 0.7778 - val_loss: 109.3934 - val_accuracy: 0.3333\nEpoch 11/20\n1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 24.4653 - val_accuracy: 0.6667\nEpoch 12/20\n1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\nEpoch 13/20\n1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\nEpoch 14/20\n1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\nEpoch 15/20\n1/1 [==============================] - 2s 2s/step - loss: 1.4726 - accuracy: 0.8889 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\nEpoch 16/20\n1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\nEpoch 17/20\n1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\nEpoch 18/20\n1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\nEpoch 19/20\n1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\nEpoch 20/20\n1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"}],"execution_count":4},{"source":"# Save the training accuracy in a variable named accuracy\naccuracy = history.history['accuracy'][-1]\n\nprint(f'Training Accuracy: {accuracy}')","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1731584917258,"lastExecutedByKernel":"041e41a6-a0a1-4d6b-89ae-5926b6ecaa91","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Save the training accuracy in a variable named accuracy\naccuracy = history.history['accuracy'][-1]\n\nprint(f'Training Accuracy: {accuracy}')","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"959aa156-9809-4151-9af2-0d0075a81fc1","outputs":[{"output_type":"stream","name":"stdout","text":"Training Accuracy: 1.0\n"}],"execution_count":5}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}