{"cells":[{"cell_type":"markdown","id":"b1309988-b429-4fb0-8c4c-193582dbec93","metadata":{},"source":["![mobydick](mobydick.jpg)"]},{"cell_type":"markdown","id":"611e416c-70e7-478a-a3c8-e54f3fdb4a7f","metadata":{},"source":["In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n","\n","The Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg."]},{"cell_type":"code","execution_count":45,"id":"15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":49,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1695294176831,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... \n","outputsMetadata":{"0":{"height":57,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["# Import and download packages\n","import requests\n","from bs4 import BeautifulSoup\n","import nltk\n","from collections import Counter\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":46,"id":"1ae08e22-6925-492b-bdf6-6926c7043d83","metadata":{"executionCancelledAt":null,"executionTime":107,"lastExecutedAt":1695294176938,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# URL of the HTML file\nurl = \"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\"\n\n# Send an HTTP GET request to the URL\nr = requests.get(url)\nr.encoding = 'utf-8'\n"},"outputs":[],"source":["# URL of the HTML file\n","url = \"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\"\n","\n","# Send an HTTP GET request to the URL\n","r = requests.get(url)\n","r.encoding = 'utf-8'"]},{"cell_type":"code","execution_count":47,"id":"eb14eba2-611b-4b13-b826-704e2c298a75","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1695294176988,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"html = r.text\nprint(html[:2000])","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["<?xml version=\"1.0\" encoding=\"utf-8\"?>\n","\n","<!DOCTYPE html\n","   PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n","   \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\" >\n","\n","<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n","  <head>\n","    <title>\n","      Moby Dick; Or the Whale, by Herman Melville\n","    </title>\n","    <style type=\"text/css\" xml:space=\"preserve\">\n","\n","    body { background:#faebd0; color:black; margin-left:15%; margin-right:15%; text-align:justify }\n","    P { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\n","    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\n","    hr  { width: 50%; text-align: center;}\n","    .foot { margin-left: 20%; margin-right: 20%; text-align: justify; text-indent: -3em; font-size: 90%; }\n","    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\n","    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\n","    .toc       { margin-left: 10%; margin-bottom: .75em;}\n","    .toc2      { margin-left: 20%;}\n","    div.fig    { display:block; margin:0 auto; text-align:center; }\n","    div.middle { margin-left: 20%; margin-right: 20%; text-align: justify; }\n","    .figleft   {float: left; margin-left: 0%; margin-right: 1%;}\n","    .figright  {float: right; margin-right: 0%; margin-left: 1%;}\n","    .pagenum   {display:inline; font-size: 70%; font-style:normal;\n","               margin: 0; padding: 0; position: absolute; right: 1%;\n","               text-align: right;}\n","    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\n","\n","    table      {margin-left: 10%;}\n","\n","a:link {color:blue;\n","\t\ttext-decoration:none}\n","link {color:blue;\n","\t\ttext-decoration:none}\n","a:visited {color:blue;\n","\t\ttext-decoration:none}\n","a:hover {color:red}\n","\n","</style>\n","  </head>\n","  <body>\n","<pre xml:space=\"preserve\">\n","\n","The Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville\n","\n","This eBook is for the use of anyone anywh\n"]}],"source":["# Extract text\n","html = r.text\n","print(html[:2000])"]},{"cell_type":"code","execution_count":48,"id":"f78dd1c3-a98d-4026-8502-4e1dc599f042","metadata":{"executionCancelledAt":null,"executionTime":223,"lastExecutedAt":1695294177211,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"html_soup = BeautifulSoup(html, 'html.parser')\nmoby_text = html_soup.get_text()"},"outputs":[],"source":["# Create soup and extract text elements\n","html_soup = BeautifulSoup(html, 'html.parser')\n","moby_text = html_soup.get_text()"]},{"cell_type":"code","execution_count":49,"id":"97ded20d-6906-46e5-b694-1c7ca84ee912","metadata":{"executionCancelledAt":null,"executionTime":87,"lastExecutedAt":1695294177299,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Initialize a regex tokenizer to keep only alphanumeric text\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n \n# Tokenize the text\ntokens = tokenizer.tokenize(moby_text)"},"outputs":[],"source":["# Initialize a regex tokenizer to keep only alphanumeric text\n","tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n"," \n","# Tokenize the text\n","tokens = tokenizer.tokenize(moby_text)"]},{"cell_type":"code","execution_count":50,"id":"6649fe8d-06ea-4a21-a659-4231500c80ae","metadata":{"executionCancelledAt":null,"executionTime":63,"lastExecutedAt":1695294177363,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Convert tokens to lowercase\nwords = [token.lower() for token in tokens]\n\nprint(words[:8])","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["['moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']\n"]}],"source":["# Convert tokens to lowercase\n","words = [token.lower() for token in tokens]\n","\n","print(words[:8])"]},{"cell_type":"code","execution_count":51,"id":"5aea99d8-d421-48b3-a8fd-90ffac9bbd07","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1695294177411,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Get English stop words\nstop_words = nltk.corpus.stopwords.words(\"english\")\nprint(stop_words[:8])","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves']\n"]}],"source":["# Get English stop words\n","stop_words = nltk.corpus.stopwords.words(\"english\")\n","print(stop_words[:8])"]},{"cell_type":"code","execution_count":52,"id":"ecac197a-0eed-4068-aad9-a4d8a7e2c9a5","metadata":{"executionCancelledAt":null,"executionTime":371,"lastExecutedAt":1695294177782,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Remove stop words\nwords_no_stop = [word for word in words if word not in stop_words]\nprint(words_no_stop[:5])","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["['moby', 'dick', 'whale', 'herman', 'melville']\n"]}],"source":["# Remove stop words\n","words_no_stop = [word for word in words if word not in stop_words]\n","print(words_no_stop[:5])"]},{"cell_type":"code","execution_count":53,"id":"3b87acab-c2bb-4216-8711-f8c73e182519","metadata":{"executionCancelledAt":null,"executionTime":44,"lastExecutedAt":1695294177826,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Initialize a Counter object to count word frequencies\ncount = Counter(words_no_stop)\n\n# Find the ten most common words\ntop_ten = word_counts.most_common(10)"},"outputs":[],"source":["# Initialize a Counter object to count word frequencies\n","count = Counter(words_no_stop)\n","\n","# Find the ten most common words\n","top_ten = word_counts.most_common(10)"]},{"cell_type":"code","execution_count":54,"id":"a37d798c-5f59-47e7-aa3d-f24f5669336c","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1695294177875,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(top_ten)","outputsMetadata":{"0":{"height":57,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["[('whale', 1246), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"]}],"source":["# Print top_ten\n","print(top_ten)"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
